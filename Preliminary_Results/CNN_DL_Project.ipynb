{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14343391,"sourceType":"datasetVersion","datasetId":9158280}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1240.313128,"end_time":"2025-12-30T14:03:06.48965","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-30T13:42:26.176522","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nadaarfaoui/amazon-electronics-visual-search-with-cnn?scriptVersionId=289233174\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"**Problem:** Directly classifying thousands of product SKUs is difficult — many items look similar and per-SKU accuracy suffers.\n\n**Solution (two stages):**\n\n1. **Brand classification (coarse):** Train a CNN to predict the product brand (fewer classes, easier to learn).\n2. **Similarity search (fine):** Within the predicted brand, compare embeddings (cosine similarity) to find the most visually similar product.\n\n**Why it works:** Fewer classes for the CNN (brand-level) and an embedding-based nearest-neighbor step for fine-grained retrieval — this improves accuracy and scalability.\n\n**When to use:** When the dataset has many visually-similar product variants and a brand-level filter meaningfully reduces search space.","metadata":{"papermill":{"duration":0.005561,"end_time":"2025-12-30T13:42:29.702994","exception":false,"start_time":"2025-12-30T13:42:29.697433","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"/kaggle/input/merged-amazon-electronics-dataset/merged_electronics_dataset.csv\", on_bad_lines='skip')\n\nprint(df.head())\nprint(df.tail())\nprint(df.shape)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:42:29.711237Z","iopub.status.busy":"2025-12-30T13:42:29.710731Z","iopub.status.idle":"2025-12-30T13:42:31.146566Z","shell.execute_reply":"2025-12-30T13:42:31.145824Z"},"id":"o9fL0PeKugJ5","outputId":"e488aab2-802c-4118-9f15-5d52a85f27ac","papermill":{"duration":1.441828,"end_time":"2025-12-30T13:42:31.148397","exception":false,"start_time":"2025-12-30T13:42:29.706569","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.columns)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:42:31.158063Z","iopub.status.busy":"2025-12-30T13:42:31.157656Z","iopub.status.idle":"2025-12-30T13:42:31.161647Z","shell.execute_reply":"2025-12-30T13:42:31.160828Z"},"papermill":{"duration":0.009879,"end_time":"2025-12-30T13:42:31.16292","exception":false,"start_time":"2025-12-30T13:42:31.153041","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create 'brand' column by taking the first word of 'name'\ndf['brand'] = df['name'].apply(lambda x: str(x).split()[0])\n\n# Check\nprint(df[['name', 'brand']].head())\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:42:31.171819Z","iopub.status.busy":"2025-12-30T13:42:31.171318Z","iopub.status.idle":"2025-12-30T13:42:31.196187Z","shell.execute_reply":"2025-12-30T13:42:31.19545Z"},"papermill":{"duration":0.030568,"end_time":"2025-12-30T13:42:31.197617","exception":false,"start_time":"2025-12-30T13:42:31.167049","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.columns)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:42:31.207069Z","iopub.status.busy":"2025-12-30T13:42:31.206663Z","iopub.status.idle":"2025-12-30T13:42:31.210572Z","shell.execute_reply":"2025-12-30T13:42:31.209796Z"},"papermill":{"duration":0.010276,"end_time":"2025-12-30T13:42:31.21204","exception":false,"start_time":"2025-12-30T13:42:31.201764","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean = df.dropna(subset=['image', 'brand','name']).reset_index(drop=True)\nprint(df_clean.shape)","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:42:31.221192Z","iopub.status.busy":"2025-12-30T13:42:31.220761Z","iopub.status.idle":"2025-12-30T13:42:31.236835Z","shell.execute_reply":"2025-12-30T13:42:31.23627Z"},"papermill":{"duration":0.022075,"end_time":"2025-12-30T13:42:31.238358","exception":false,"start_time":"2025-12-30T13:42:31.216283","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean = df_clean[['brand', 'image','name']]\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:42:31.247781Z","iopub.status.busy":"2025-12-30T13:42:31.247251Z","iopub.status.idle":"2025-12-30T13:42:31.25145Z","shell.execute_reply":"2025-12-30T13:42:31.250964Z"},"id":"XPTXcdLOZG4C","papermill":{"duration":0.010252,"end_time":"2025-12-30T13:42:31.252724","exception":false,"start_time":"2025-12-30T13:42:31.242472","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#download images\nimport os\nimport requests\nfrom tqdm import tqdm\n\n# Folder to save images\nimage_dir = '/kaggle/working/images'\nos.makedirs(image_dir, exist_ok=True)\n\n# Download images\ndf_clean['image_path'] = None  # new column for local image path\n\nfor idx, row in tqdm(df_clean.iterrows(), total=df_clean.shape[0]):\n    url = row['image']\n    if pd.isna(url):\n        continue\n    try:\n        response = requests.get(url, timeout=5)\n        ext = url.split('.')[-1].split('?')[0]  # get jpg/png extension\n        file_path = os.path.join(image_dir, f\"{idx}.{ext}\")\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n        df_clean.at[idx, 'image_path'] = file_path\n    except:\n        pass\n\n# Keep only rows where download succeeded\ndf_clean = df_clean.dropna(subset=['image_path']).reset_index(drop=True)\nprint(df_clean.shape)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:42:31.262071Z","iopub.status.busy":"2025-12-30T13:42:31.26181Z","iopub.status.idle":"2025-12-30T13:58:10.615099Z","shell.execute_reply":"2025-12-30T13:58:10.614257Z"},"id":"u9hTWZc9vrVX","outputId":"d13882a3-cd78-4ea4-e7dd-532a076bbe28","papermill":{"duration":939.359412,"end_time":"2025-12-30T13:58:10.616746","exception":false,"start_time":"2025-12-30T13:42:31.257334","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm import tqdm\n\n# Correct imports for loading and converting images\nfrom tensorflow.keras.utils import load_img, img_to_array\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:58:11.045638Z","iopub.status.busy":"2025-12-30T13:58:11.04537Z","iopub.status.idle":"2025-12-30T13:58:37.850733Z","shell.execute_reply":"2025-12-30T13:58:37.849987Z"},"id":"K5r4ExBie_Y9","papermill":{"duration":27.055234,"end_time":"2025-12-30T13:58:37.852563","exception":false,"start_time":"2025-12-30T13:58:10.797329","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode brands\nle_brand = LabelEncoder()\ndf_clean['brand_encoded'] = le_brand.fit_transform(df_clean['brand'])\nnum_brands = df_clean['brand_encoded'].nunique()\nprint(\"Number of brands:\", num_brands)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:58:38.198347Z","iopub.status.busy":"2025-12-30T13:58:38.197749Z","iopub.status.idle":"2025-12-30T13:58:38.212507Z","shell.execute_reply":"2025-12-30T13:58:38.211864Z"},"id":"I7q560wrYUPA","outputId":"a1506115-bf02-4316-88a6-f9ea3b0f8ada","papermill":{"duration":0.185967,"end_time":"2025-12-30T13:58:38.213832","exception":false,"start_time":"2025-12-30T13:58:38.027865","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert brand_encoded to strings\ndf_clean['brand_encoded_str'] = df_clean['brand_encoded'].astype(str)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:58:38.894537Z","iopub.status.busy":"2025-12-30T13:58:38.894282Z","iopub.status.idle":"2025-12-30T13:58:38.899441Z","shell.execute_reply":"2025-12-30T13:58:38.898881Z"},"id":"E2kRZZVEimam","papermill":{"duration":0.17375,"end_time":"2025-12-30T13:58:38.900732","exception":false,"start_time":"2025-12-30T13:58:38.726982","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\nvalid_paths = []\nfor path in df_clean['image_path']:\n    try:\n        img = Image.open(path)\n        img.verify()  # checks if image can be opened\n        valid_paths.append(path)\n    except:\n        pass\n\n# Keep only rows with valid images\ndf_clean = df_clean[df_clean['image_path'].isin(valid_paths)].reset_index(drop=True)\nprint(\"Number of valid images:\", len(df_clean))\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:58:39.24005Z","iopub.status.busy":"2025-12-30T13:58:39.239757Z","iopub.status.idle":"2025-12-30T13:58:39.954434Z","shell.execute_reply":"2025-12-30T13:58:39.953748Z"},"id":"gFQUWya5jcak","outputId":"fa5c4615-4e17-405f-ad10-63b9362d2525","papermill":{"duration":0.884465,"end_time":"2025-12-30T13:58:39.955982","exception":false,"start_time":"2025-12-30T13:58:39.071517","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Re-encode brands after filtering valid images\nfrom sklearn.preprocessing import LabelEncoder\nle_brand = LabelEncoder()\ndf_clean['brand_encoded'] = le_brand.fit_transform(df_clean['brand'])\ndf_clean['brand_encoded_str'] = df_clean['brand_encoded'].astype(str)\nnum_brands = df_clean['brand_encoded'].nunique()\nprint('Number of brands after re-encoding:', num_brands)\n\n# Prepare image data generators\n# Data augmentation for brand classification\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,  # 80% train, 20% validation\n    horizontal_flip=True,\n    rotation_range=20,\n    zoom_range=0.2\n)\n\ntrain_gen = datagen.flow_from_dataframe(\n    dataframe=df_clean,\n    x_col='image_path',\n    y_col='brand_encoded_str',  # use string labels\n    target_size=(224,224),\n    batch_size=32,\n    class_mode='categorical',   # categorical works with string labels\n    subset='training'\n)\n\nval_gen = datagen.flow_from_dataframe(\n    dataframe=df_clean,\n    x_col='image_path',\n    y_col='brand_encoded_str',\n    target_size=(224,224),\n    batch_size=32,\n    class_mode='categorical',\n    subset='validation'\n)\n\n#CNN will learn to classify brands, not individual products yet.","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:58:40.377001Z","iopub.status.busy":"2025-12-30T13:58:40.376421Z","iopub.status.idle":"2025-12-30T13:58:40.416146Z","shell.execute_reply":"2025-12-30T13:58:40.415485Z"},"id":"OhG05XINgu4b","outputId":"bad52f40-5ec2-405c-ee16-e5a3376afcaa","papermill":{"duration":0.284876,"end_time":"2025-12-30T13:58:40.417558","exception":false,"start_time":"2025-12-30T13:58:40.132682","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#build the cnn transfer learning\n# Pretrained CNN as feature extractor\nbase_model = EfficientNetB0(include_top=False, input_shape=(224,224,3), weights='imagenet')\nbase_model.trainable = False  # freeze pretrained layers\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.5)(x)\noutput = Dense(num_brands, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=output)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:58:40.763495Z","iopub.status.busy":"2025-12-30T13:58:40.762843Z","iopub.status.idle":"2025-12-30T13:58:46.187319Z","shell.execute_reply":"2025-12-30T13:58:46.186616Z"},"id":"3cpvbMvei04s","outputId":"9c3469b9-4590-4ddf-d617-c099b249f666","papermill":{"duration":5.602952,"end_time":"2025-12-30T13:58:46.194324","exception":false,"start_time":"2025-12-30T13:58:40.591372","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After filtering invalid images\nnum_brands = df_clean['brand_encoded_str'].nunique()\nprint(\"Number of brands after filtering:\", num_brands)\n\n# Rebuild the model output layer\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nbase_model = EfficientNetB0(include_top=False, input_shape=(224,224,3), weights='imagenet')\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.5)(x)\noutput = Dense(num_brands, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=output)\n\n# Compile\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:58:46.550635Z","iopub.status.busy":"2025-12-30T13:58:46.550081Z","iopub.status.idle":"2025-12-30T13:58:47.489112Z","shell.execute_reply":"2025-12-30T13:58:47.488319Z"},"id":"tgw2vj5-j-GB","outputId":"352d35fd-dae5-4988-b8c3-1d40b18c7f13","papermill":{"duration":1.121277,"end_time":"2025-12-30T13:58:47.490812","exception":false,"start_time":"2025-12-30T13:58:46.369535","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=10,\n    steps_per_epoch=train_gen.samples // train_gen.batch_size,\n    validation_steps=val_gen.samples // val_gen.batch_size\n)\n\n#After training, the CNN can predict the brand from a new product image","metadata":{"execution":{"iopub.execute_input":"2025-12-30T13:58:47.842808Z","iopub.status.busy":"2025-12-30T13:58:47.841998Z","iopub.status.idle":"2025-12-30T14:02:58.923022Z","shell.execute_reply":"2025-12-30T14:02:58.92223Z"},"id":"cg2H9_Jri6Nq","outputId":"3607b047-282b-434c-bd4a-fc6b08c0add7","papermill":{"duration":251.257719,"end_time":"2025-12-30T14:02:58.924778","exception":false,"start_time":"2025-12-30T13:58:47.667059","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the entire CNN model (architecture + weights)\nmodel.save('/kaggle/working/brand_cnn_model.h5')\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T14:02:59.295521Z","iopub.status.busy":"2025-12-30T14:02:59.294907Z","iopub.status.idle":"2025-12-30T14:03:00.017491Z","shell.execute_reply":"2025-12-30T14:03:00.016647Z"},"id":"sc07zJ27lPWC","outputId":"272d7528-73e0-4551-ae09-c3e3821accdb","papermill":{"duration":0.907897,"end_time":"2025-12-30T14:03:00.019355","exception":false,"start_time":"2025-12-30T14:02:59.111458","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Save LabelEncoder / artifacts to Kaggle working directory\nwith open('/kaggle/working/brand_encoder.pkl', 'wb') as f:\n    pickle.dump(le_brand, f)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T14:03:00.388016Z","iopub.status.busy":"2025-12-30T14:03:00.387536Z","iopub.status.idle":"2025-12-30T14:03:00.391951Z","shell.execute_reply":"2025-12-30T14:03:00.391443Z"},"id":"9zaRe6MXlXJr","outputId":"c67227a1-b6c4-4368-e9db-c00830f5129d","papermill":{"duration":0.188159,"end_time":"2025-12-30T14:03:00.393354","exception":false,"start_time":"2025-12-30T14:03:00.205195","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CNN for Brand Classification**\n\n- **Backbone:** EfficientNetB0 (pretrained on ImageNet).\n- **Head:** GlobalAveragePooling2D → Dropout → Dense(softmax).\n- **Task:** predict the product *brand* (not individual product SKUs).\n\n**Notes:** Transfer learning provides strong visual features from the pretrained backbone; the classification head (pooling, dropout, dense) is trained on your brand labels.\n\n**Summary:** the model uses a pretrained convolutional backbone with a custom classification head for brand-level prediction.","metadata":{"id":"zedLlc9gS5nD","papermill":{"duration":0.183421,"end_time":"2025-12-30T14:03:00.760861","exception":false,"start_time":"2025-12-30T14:03:00.57744","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\nvalid_files = []\nfor filename in os.listdir('/kaggle/working/images'):\n    path = os.path.join('/kaggle/working/images', filename)\n    try:\n        img = Image.open(path)\n        img.verify()  # Check if image is readable\n        valid_files.append(path)\n    except:\n        print(f\"Removing corrupted image: {path}\")\n        os.remove(path)  # Delete corrupted file immediately\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T14:03:01.20386Z","iopub.status.busy":"2025-12-30T14:03:01.203147Z","iopub.status.idle":"2025-12-30T14:03:01.773073Z","shell.execute_reply":"2025-12-30T14:03:01.772333Z"},"id":"k7kvrNy7qDRr","outputId":"b2069ddc-623e-4e15-8a85-9fd90b934c0d","papermill":{"duration":0.830521,"end_time":"2025-12-30T14:03:01.775163","exception":false,"start_time":"2025-12-30T14:03:00.944642","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_clean = df_clean[df_clean['image_path'].isin(valid_files)].reset_index(drop=True)\nprint(\"Remaining valid images:\", df_clean.shape[0])\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T14:03:02.151097Z","iopub.status.busy":"2025-12-30T14:03:02.150418Z","iopub.status.idle":"2025-12-30T14:03:02.157076Z","shell.execute_reply":"2025-12-30T14:03:02.1564Z"},"id":"O15LuTdRqG1z","papermill":{"duration":0.19478,"end_time":"2025-12-30T14:03:02.158547","exception":false,"start_time":"2025-12-30T14:03:01.963767","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keep required columns including image_path and brand for downstream steps\ndf_clean = df_clean[['name', 'image', 'image_path', 'brand']].copy()\nprint(df_clean.head())\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T14:03:02.542354Z","iopub.status.busy":"2025-12-30T14:03:02.541634Z","iopub.status.idle":"2025-12-30T14:03:02.548537Z","shell.execute_reply":"2025-12-30T14:03:02.547717Z"},"id":"HDwO6sYPqZoM","papermill":{"duration":0.195734,"end_time":"2025-12-30T14:03:02.549826","exception":false,"start_time":"2025-12-30T14:03:02.354092","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Keep only rows with actual image files\ndf_clean = df_clean[df_clean['image_path'].apply(os.path.isfile)].reset_index(drop=True)\n\nprint(\"Remaining images:\", df_clean.shape[0])\n","metadata":{"execution":{"iopub.execute_input":"2025-12-30T14:03:02.926179Z","iopub.status.busy":"2025-12-30T14:03:02.925865Z","iopub.status.idle":"2025-12-30T14:03:03.246271Z","shell.execute_reply":"2025-12-30T14:03:03.245301Z"},"id":"0_OrVkZGrH9s","papermill":{"duration":0.51022,"end_time":"2025-12-30T14:03:03.247437","exception":true,"start_time":"2025-12-30T14:03:02.737217","status":"failed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 5: Extract image embeddings for similarity search\n#We will create feature vectors for all images to compare images within the same brand.\n\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\n\nembedding_model = Model(inputs=base_model.input, outputs=GlobalAveragePooling2D()(base_model.output))\nembeddings = {}\n\nfor idx, row in tqdm(df_clean.iterrows(), total=df_clean.shape[0]):\n    img_path = row['image_path']\n    img = image.load_img(img_path, target_size=(224,224))\n    img_array = image.img_to_array(img)/255.0\n    img_array = np.expand_dims(img_array, axis=0)\n    emb = embedding_model.predict(img_array, verbose=0)\n    # Key embeddings by image_path to remain stable across filtering\n    embeddings[row['image_path']] = emb.flatten()\n","metadata":{"id":"YbzLErNZlzN7","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Predict brand + find most similar product\n\ndef predict_product(img_path, top_k=1):\n    # 1️⃣ Predict brand\n    img = image.load_img(img_path, target_size=(224,224))\n    x = image.img_to_array(img)/255.0\n    x = np.expand_dims(x, axis=0)\n\n    brand_pred = model.predict(x)#brand_pred: array of size (1, num_brands) → probability for each brand.\n    brand_idx = np.argmax(brand_pred)#brand_idx: integer → index of predicted brand.\n    brand_name = le_brand.inverse_transform([brand_idx])[0] #brand_name: string → the predicted brand.\n\n    # 2️⃣ Find embeddings of products with the same brand (use image_path as key)\n    brand_products = df_clean[df_clean['brand'] == brand_name][['name','image_path']].reset_index(drop=True)\n    brand_embeddings = np.array([embeddings[path] for path in brand_products['image_path']])\n\n    # 3️⃣ Compute similarity with input image embedding\n    img_emb = embedding_model.predict(x).flatten().reshape(1, -1)\n    similarities = cosine_similarity(img_emb, brand_embeddings).flatten()\n\n    # 4️⃣ Pick most similar product\n    best_idx = np.argmax(similarities)\n    predicted_product = brand_products.loc[best_idx, 'name']\n\n    return brand_name, predicted_product\n","metadata":{"id":"uHLeRHqIoLBL","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure brand column exists on the original dataframe (don't overwrite df_clean here)\ndf['brand'] = df['name'].apply(lambda x: str(x).split()[0])\n","metadata":{"id":"UFpRUWqQsnB8","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"brand, product = predict_product('/kaggle/working/images/0.jpg')\nprint(\"Predicted brand:\", brand)\nprint(\"Predicted product:\", product)\n","metadata":{"id":"x0ZAaIxKoLE-","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('/kaggle/working/brand_classifier.h5')\n","metadata":{"id":"MWRLEPZAsx6U","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/working/le_brand.pkl', 'wb') as f:\n    pickle.dump(le_brand, f)\n","metadata":{"id":"oKgUfMy2s-Wt","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Pipeline Overview**\n\n1. Input image\n2. CNN predicts the brand\n3. Filter dataset to products of the predicted brand\n4. Compute cosine similarity between image embeddings\n5. Return the most visually similar product\n\nThis hybrid pipeline reduces the search space (brand → similarity) for more accurate fine-grained retrieval.","metadata":{"id":"A1vRpnyutrnX","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"**Hybrid Approach — Brand Classification + Embedding Similarity**\n\n**Goal:** Given a product image, identify the exact product efficiently and robustly.\n\n**Problem (direct per-SKU classification):**\n- Datasets contain thousands of product SKUs; many look visually similar.\n- Training a CNN to classify every single SKU is slow, memory-intensive, and often low accuracy.\n\n**Solution (two-stage hybrid):**\n1. **Brand classification (coarse):** a CNN predicts the product brand (far fewer classes).\n2. **Similarity search (fine):** within the predicted brand, compare image embeddings using cosine similarity to find the most visually similar product.\n\n**Why this works:**\n- Reduces the search space (brand → product), making nearest-neighbor retrieval tractable.\n- Leverages pretrained backbones to extract strong visual features; embeddings capture fine-grained differences.\n\n**When to use / benefits:**\n- Useful when many SKUs per brand exist and brand acts as a meaningful filter.\n- Easier to scale and update: add new products by computing embeddings rather than retraining the full classifier.\n\n**Example:** CNN predicts \n → limit candidates to Samsung products → return the product with highest cosine similarity in embedding space.","metadata":{"id":"qkFq-cVVtvtO","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"**Results & Next Steps**\n\n- Current results were not satisfactory with the initial direct approach.\n- Switched to the hybrid pipeline: brand classification + embedding similarity retrieval.\n- Next: evaluate retrieval accuracy, tune embedding model, and reduce noisy images.","metadata":{"id":"jVunBH5uPFkM","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"**Key Benefits of the Hybrid Pipeline**\n\n- Extracts deep visual features via a pretrained backbone.\n- Embeds all product images into a common vector space for comparison.\n- Uses cosine similarity for efficient nearest-neighbor retrieval.\n- Predicts brand (CNN) and then the specific product (embedding search).","metadata":{"id":"RJ780zNgN_PN","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"import os, glob, pickle\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ---------------- Configuration ----------------\nCSV_PATH = \"/kaggle/input/merged-amazon-electronics-dataset/merged_electronics_dataset.csv\"\nIMAGE_DIR = \"/kaggle/working/images\"\nINPUT_SIZE = 300\nRANDOM_SEED = 42\n\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# ---------------- Load CSV ----------------\ndf = pd.read_csv(CSV_PATH, on_bad_lines='skip')\nif 'name' not in df.columns or 'image' not in df.columns:\n    raise ValueError(\"CSV must contain 'name' and 'image' columns.\")\n\n# ---------------- Map CSV rows to existing images ----------------\nexisting_images = sorted(glob.glob(os.path.join(IMAGE_DIR, \"*.jpg\")))  # adjust extension if needed\ndf = df.iloc[:len(existing_images)].copy()\ndf['image_path'] = existing_images\n\n# ---------------- Extract brand ----------------\ndf['brand'] = df['name'].astype(str).apply(lambda x: str(x).split()[0].strip())\ndf = df.dropna(subset=['name','brand','image_path']).reset_index(drop=True)\nprint(\"Images and rows:\", len(df))\n\n# ---------------- Build embedding model ----------------\nbase_model = EfficientNetB3(include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3), weights='imagenet')\nembedding_output = GlobalAveragePooling2D()(base_model.output)\nembedding_model = Model(inputs=base_model.input, outputs=embedding_output)\nprint(\"Embedding model created.\")\n\n# ---------------- Create embeddings ----------------\nprint(\"Creating embeddings for all product images...\")\nembeddings = {}\nproduct_rows = []\n\nfor _, row in tqdm(df.iterrows(), total=len(df)):\n    try:\n        img = Image.open(row['image_path']).convert('RGB').resize((INPUT_SIZE, INPUT_SIZE))\n        arr = np.expand_dims(preprocess_input(np.array(img, dtype=np.float32)), 0)\n        emb = embedding_model.predict(arr, verbose=0)\n        embeddings[row['name']] = emb.flatten()\n        product_rows.append({'name': row['name'], 'brand': row['brand'], 'image_path': row['image_path']})\n    except:\n        continue\n\nwith open(\"/kaggle/working/embeddings.pkl\", \"wb\") as f:\n    pickle.dump(embeddings, f)\n\nproducts_df = pd.DataFrame(product_rows)\nproducts_df.to_csv(\"/kaggle/working/products_index.csv\", index=False)\nprint(\"Saved embeddings and product index.\")\n\n# ---------------- Prediction function (top-1) ----------------\ndef predict_product(img_path):\n    img = Image.open(img_path).convert('RGB').resize((INPUT_SIZE, INPUT_SIZE))\n    arr = np.expand_dims(preprocess_input(np.array(img, dtype=np.float32)), 0)\n    emb = embedding_model.predict(arr, verbose=0).reshape(1, -1)\n\n    # Compare with all products\n    candidate_names = list(embeddings.keys())\n    candidate_embs = np.array([embeddings[n] for n in candidate_names])\n    sims = cosine_similarity(emb, candidate_embs).flatten()\n\n    top_idx = sims.argmax()  # top-1\n    top_name = candidate_names[top_idx]\n    top_brand = products_df[products_df['name'] == top_name]['brand'].values[0]\n    top_score = float(sims[top_idx])\n\n    return top_brand, top_name, top_score\n\n# ---------------- Quick test ----------------\nsample_image = sorted(glob.glob(os.path.join(IMAGE_DIR, \"*.jpg\")))[0]\nbrand, name, score = predict_product(sample_image)\n\nprint(\"Sample prediction:\")\nprint(\"Brand:\", brand)\nprint(\"Product Name:\", name)\nprint(\"Similarity:\", score)\n","metadata":{"id":"en8OCsyuN_ZT","outputId":"e6119d59-53f5-43c6-bf7a-f02c0bb7aa8f","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"execution_count":null}]}