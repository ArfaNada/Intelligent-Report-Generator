{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14343391,"sourceType":"datasetVersion","datasetId":9158280},{"sourceId":171647,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":146094,"modelId":164048}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nadaarfaoui/summary-of-amazon-comments?scriptVersionId=289238806\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-11-25T17:39:05.238702Z","iopub.status.busy":"2025-11-25T17:39:05.23854Z","iopub.status.idle":"2025-11-25T17:39:08.538234Z","shell.execute_reply":"2025-11-25T17:39:08.537351Z","shell.execute_reply.started":"2025-11-25T17:39:05.238684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# FAST Review Summarization with BART/T5 + Qwen2.5 Narrative Enhancement\n# Optimized for Kaggle - Auto-handles GPTQ and fallback models\n# Modified for electronics dataset with review_text column\n# INCREMENTAL SAVING: Appends every 100 products to build final file step by step\n# ============================================================================\n\nimport pandas as pd\nimport torch\nfrom transformers import (\n    BartForConditionalGeneration, BartTokenizer,\n    T5ForConditionalGeneration, T5Tokenizer,\n    AutoModelForCausalLM, AutoTokenizer\n)\nfrom rouge_score import rouge_scorer\nfrom tqdm import tqdm\nimport warnings\nimport time\nimport os\nwarnings.filterwarnings('ignore')\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nSAMPLE_SIZE = None  # Set to None to process all products\nUSE_NARRATIVE_LLM = True  # Enable Qwen for narrative generation\nBATCH_SIZE = 100  # Append to file every 100 products\nCOMPLETE_RESULTS_FILE = 'complete_results.csv'\nFINAL_NARRATIVES_FILE = 'final_narratives.csv'\n\n# Remove existing files to start fresh\nif os.path.exists(COMPLETE_RESULTS_FILE):\n    os.remove(COMPLETE_RESULTS_FILE)\n    print(f\"Removed existing {COMPLETE_RESULTS_FILE}\")\nif os.path.exists(FINAL_NARRATIVES_FILE):\n    os.remove(FINAL_NARRATIVES_FILE)\n    print(f\"Removed existing {FINAL_NARRATIVES_FILE}\")\n\n# ============================================================================\n# LOAD DATASET\n# ============================================================================\nprint(\"\\nLoading dataset...\")\ndf = pd.read_csv('/kaggle/input/merged-amazon-electronics-dataset/merged_electronics_dataset.csv')\n\n# Handle missing review_text - keep rows but mark them\ndf['has_review'] = df['review_text'].notna() & (df['review_text'].astype(str).str.strip() != '') & (df['review_text'].astype(str).str.strip() != 'nan')\ndf['review_text'] = df['review_text'].fillna('')\ndf['review_text'] = df['review_text'].astype(str)\n\n# Filter out empty reviews for processing\ndf_with_reviews = df[df['has_review']].copy()\ndf_without_reviews = df[~df['has_review']].copy()\n\nprint(f\"Total products: {len(df)}\")\nprint(f\"Products with reviews: {len(df_with_reviews)}\")\nprint(f\"Products without reviews: {len(df_without_reviews)}\")\n\n# ============================================================================\n# LOAD PRE-TRAINED SUMMARIZATION MODELS\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING PRE-TRAINED SUMMARIZATION MODELS\")\nprint(\"=\"*80)\n\nprint(\"\\nLoading pre-trained BART...\")\nbart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nbart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\nbart_model.to(device)\nbart_model.eval()\nprint(\"✓ Pre-trained BART loaded\")\n\nprint(\"\\nLoading pre-trained T5...\")\nt5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\nt5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\nt5_model.to(device)\nt5_model.eval()\nprint(\"✓ Pre-trained T5 loaded\")\n\n# ============================================================================\n# LOAD QWEN2.5-1.5B-INSTRUCT (AUTO-DETECT BEST VERSION)\n# ============================================================================\nnarrative_model = None\nnarrative_tokenizer = None\n\nif USE_NARRATIVE_LLM:\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING QWEN2.5-1.5B-INSTRUCT FOR NARRATIVE GENERATION\")\n    print(\"=\"*80)\n    \n    # Strategy: Try local GPTQ → Try local non-GPTQ → Download from HuggingFace\n    \n    # Option 1: Try local GPTQ model\n    try:\n        local_gptq_path = \"/kaggle/input/qwen2.5/transformers/1.5b-instruct-gptq-int8/1\"\n        print(f\"Attempting to load local GPTQ model...\")\n        \n        # Try to import optimum\n        try:\n            import optimum\n            print(\"  ✓ optimum available\")\n            use_gptq = True\n        except ImportError:\n            print(\"  ⚠️ optimum not available, will try standard loading\")\n            use_gptq = False\n        \n        narrative_tokenizer = AutoTokenizer.from_pretrained(local_gptq_path, trust_remote_code=True)\n        \n        if use_gptq:\n            narrative_model = AutoModelForCausalLM.from_pretrained(\n                local_gptq_path,\n                device_map=\"auto\",\n                trust_remote_code=True,\n                low_cpu_mem_usage=True\n            )\n            print(\"✓ Qwen2.5-1.5B-Instruct loaded (LOCAL GPTQ-Int8 - FASTEST)\")\n        else:\n            # Try loading GPTQ model without optimum (might work for some formats)\n            narrative_model = AutoModelForCausalLM.from_pretrained(\n                local_gptq_path,\n                torch_dtype=torch.float16,\n                device_map=\"auto\",\n                trust_remote_code=True,\n                low_cpu_mem_usage=True\n            )\n            print(\"✓ Qwen2.5-1.5B-Instruct loaded (LOCAL float16 - FAST)\")\n            \n    except Exception as e1:\n        print(f\"  Local GPTQ loading failed: {e1}\")\n        \n        # Option 2: Try other local Qwen paths\n        try:\n            # Check if there are other Qwen model paths available\n            local_paths = [\n                \"/kaggle/input/qwen2.5/transformers/1.5b-instruct/1\",\n                \"/kaggle/input/qwen-2-5-1-5b-instruct\",\n                \"/kaggle/input/qwen2.5\"\n            ]\n            \n            for local_path in local_paths:\n                try:\n                    print(f\"  Trying alternative local path: {local_path}\")\n                    narrative_tokenizer = AutoTokenizer.from_pretrained(local_path, trust_remote_code=True)\n                    narrative_model = AutoModelForCausalLM.from_pretrained(\n                        local_path,\n                        torch_dtype=torch.float16,\n                        device_map=\"auto\",\n                        trust_remote_code=True,\n                        low_cpu_mem_usage=True\n                    )\n                    print(f\"✓ Qwen2.5-1.5B-Instruct loaded (LOCAL from {local_path})\")\n                    break\n                except:\n                    continue\n                    \n        except Exception as e2:\n            print(f\"  Alternative local paths failed: {e2}\")\n        \n        # Option 3: Download from HuggingFace (if internet available)\n        if narrative_model is None:\n            try:\n                print(\"Downloading Qwen2.5-1.5B-Instruct from HuggingFace...\")\n                hf_model = \"Qwen/Qwen2.5-1.5B-Instruct\"\n                \n                narrative_tokenizer = AutoTokenizer.from_pretrained(hf_model)\n                narrative_model = AutoModelForCausalLM.from_pretrained(\n                    hf_model,\n                    torch_dtype=torch.float16,\n                    device_map=\"auto\",\n                    low_cpu_mem_usage=True\n                )\n                print(\"✓ Qwen2.5-1.5B-Instruct loaded (HUGGINGFACE float16)\")\n                \n            except Exception as e3:\n                print(f\"  HuggingFace download failed: {e3}\")\n                print(\"  ⚠️ Could not load any Qwen model\")\n                narrative_model = None\n                narrative_tokenizer = None\n    \n    if narrative_model is not None:\n        narrative_model.eval()\n        print(\"✓ Qwen model ready for narrative generation\")\n    else:\n        print(\"⚠️ No Qwen model available - will use simple formatting\")\n\n# ============================================================================\n# PREPARE DATA FOR PROCESSING\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"PREPARING DATA FOR PROCESSING\")\nprint(\"=\"*80)\n\nif SAMPLE_SIZE is not None:\n    df_with_reviews = df_with_reviews.head(SAMPLE_SIZE)\n    print(f\"\\n{'='*80}\")\n    print(f\"⚠️  RUNNING IN TEST MODE: Processing only {SAMPLE_SIZE} products\")\n    print(f\"{'='*80}\\n\")\n\nprint(f\"Processing {len(df_with_reviews)} products with reviews\")\nprint(f\"Batch size: {BATCH_SIZE} products per save\")\nprint(f\"Files will be built incrementally:\")\nprint(f\"  - {COMPLETE_RESULTS_FILE}\")\nprint(f\"  - {FINAL_NARRATIVES_FILE}\")\n\n# ============================================================================\n# GENERATION FUNCTIONS\n# ============================================================================\ndef generate_summary(model, tokenizer, text, model_type='bart', max_length=150):\n    \"\"\"Generate summary using pre-trained model\"\"\"\n    if model_type == 't5':\n        text = \"summarize: \" + text\n    \n    inputs = tokenizer(\n        text,\n        max_length=1024,\n        truncation=True,\n        return_tensors=\"pt\",\n        padding=True\n    ).to(device)\n    \n    with torch.no_grad():\n        summary_ids = model.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_length=max_length,\n            min_length=30,\n            length_penalty=2.0,\n            num_beams=4,\n            early_stopping=True,\n            no_repeat_ngram_size=3\n        )\n    \n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\ndef clean_and_enhance_summary(raw_summary, product_name, review_rating):\n    \"\"\"Use Qwen2.5 to create professional narrative from raw summary\"\"\"\n    if narrative_model is None or narrative_tokenizer is None:\n        # Fallback: Enhanced formatting without LLM\n        return f\"Customer feedback ({review_rating}): {raw_summary}\"\n    \n    # Qwen2.5 chat format\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You transform product summaries into natural 2-3 sentence customer feedback narratives.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Product: {product_name}\\nRating: {review_rating}\\nReview: {raw_summary}\\n\\nWrite a professional customer feedback narrative using phrases like 'The customer mentioned...', 'The reviewer noted...'. Be concise and factual.\"\n        }\n    ]\n    \n    try:\n        # Apply chat template\n        text = narrative_tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        \n        model_device = next(narrative_model.parameters()).device\n        inputs = narrative_tokenizer([text], return_tensors=\"pt\", truncation=True, max_length=512).to(model_device)\n        \n        with torch.no_grad():\n            outputs = narrative_model.generate(\n                **inputs,\n                max_new_tokens=100,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True,\n                repetition_penalty=1.1,\n                pad_token_id=narrative_tokenizer.eos_token_id if narrative_tokenizer.eos_token_id else narrative_tokenizer.pad_token_id\n            )\n        \n        # Decode only new tokens\n        narrative = narrative_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n        \n        # Clean up\n        if narrative.lower().startswith((\"here\", \"sure\", \"of course\")):\n            sentences = narrative.split('.')\n            if len(sentences) > 1:\n                narrative = '.'.join(sentences[1:]).strip()\n        \n        # Fallback if too short\n        if len(narrative) < 20:\n            return f\"Customer feedback ({review_rating}): {raw_summary}\"\n        \n        return narrative\n        \n    except Exception as e:\n        return f\"Customer feedback ({review_rating}): {raw_summary}\"\n\ndef calculate_rouge(reference, hypothesis):\n    \"\"\"Calculate ROUGE scores\"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = scorer.score(reference, hypothesis)\n    return {\n        'ROUGE-1': scores['rouge1'].fmeasure,\n        'ROUGE-2': scores['rouge2'].fmeasure,\n        'ROUGE-L': scores['rougeL'].fmeasure\n    }\n\ndef select_best_summary(bart_summary, t5_summary, bart_rouge, t5_rouge):\n    \"\"\"Select best summary based on ROUGE scores\"\"\"\n    bart_avg = sum(bart_rouge.values()) / 3\n    t5_avg = sum(t5_rouge.values()) / 3\n    \n    if bart_avg >= t5_avg:\n        return {'model_name': 'BART', 'summary': bart_summary, 'avg_score': bart_avg, 'rouge_scores': bart_rouge}\n    else:\n        return {'model_name': 'T5', 'summary': t5_summary, 'avg_score': t5_avg, 'rouge_scores': t5_rouge}\n\ndef append_to_files(batch_results, is_first_batch=False):\n    \"\"\"Append batch results to the CSV files\"\"\"\n    batch_df = pd.DataFrame(batch_results)\n    \n    # Determine mode and header\n    mode = 'w' if is_first_batch else 'a'\n    header = is_first_batch\n    \n    # Append to complete results\n    batch_df.to_csv(COMPLETE_RESULTS_FILE, mode=mode, header=header, index=False)\n    \n    # Append to narratives file\n    narratives_df = batch_df[['name', 'main_category', 'sub_category', 'review_rating', 'no_of_ratings', \n                               'discount_price', 'actual_price', 'best_model', 'raw_best_summary', \n                               'cleaned_narrative', 'best_avg_score', 'link']]\n    narratives_df.to_csv(FINAL_NARRATIVES_FILE, mode=mode, header=header, index=False)\n    \n    # Get current file sizes\n    complete_size = os.path.getsize(COMPLETE_RESULTS_FILE) / (1024 * 1024)  # MB\n    narrative_size = os.path.getsize(FINAL_NARRATIVES_FILE) / (1024 * 1024)  # MB\n    \n    print(f\"  ✓ Appended {len(batch_results)} products\")\n    print(f\"    {COMPLETE_RESULTS_FILE}: {complete_size:.2f} MB\")\n    print(f\"    {FINAL_NARRATIVES_FILE}: {narrative_size:.2f} MB\")\n\n# ============================================================================\n# GENERATE SUMMARIES AND NARRATIVES (WITH INCREMENTAL SAVING)\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"GENERATING SUMMARIES AND NARRATIVES (INCREMENTAL MODE)\")\nprint(\"=\"*80)\n\ncurrent_batch = []\ntotal_processed = 0\nbatch_count = 0\nmodel_selection_count = {'BART': 0, 'T5': 0}\nstart_total = time.time()\nall_rouge_scores = {'bart': [], 't5': []}\n\nfor idx, row in tqdm(df_with_reviews.iterrows(), total=len(df_with_reviews), desc=\"Processing\"):\n    product_start = time.time()\n    review_text = str(row['review_text'])\n    \n    # Generate summaries\n    bart_summary = generate_summary(bart_model, bart_tokenizer, review_text, 'bart')\n    t5_summary = generate_summary(t5_model, t5_tokenizer, review_text, 't5')\n    \n    # Calculate ROUGE\n    sentences = review_text.split('.')\n    reference = '.'.join(sentences[:5]).strip() + '.'\n    \n    bart_rouge = calculate_rouge(reference, bart_summary)\n    t5_rouge = calculate_rouge(reference, t5_summary)\n    \n    # Track ROUGE scores\n    all_rouge_scores['bart'].append(bart_rouge)\n    all_rouge_scores['t5'].append(t5_rouge)\n    \n    # Select best\n    best = select_best_summary(bart_summary, t5_summary, bart_rouge, t5_rouge)\n    model_selection_count[best['model_name']] += 1\n    \n    # Generate narrative\n    narrative = clean_and_enhance_summary(best['summary'], row['name'], row['review_rating'])\n    \n    result = {\n        'name': row['name'],\n        'main_category': row['main_category'],\n        'sub_category': row['sub_category'],\n        'image': row['image'],\n        'link': row['link'],\n        'no_of_ratings': row['no_of_ratings'],\n        'discount_price': row['discount_price'],\n        'actual_price': row['actual_price'],\n        'review_rating': row['review_rating'],\n        'original_review_text': review_text,\n        'bart_summary': bart_summary,\n        'bart_rouge1': bart_rouge['ROUGE-1'],\n        'bart_rouge2': bart_rouge['ROUGE-2'],\n        'bart_rougeL': bart_rouge['ROUGE-L'],\n        'bart_avg_rouge': sum(bart_rouge.values()) / 3,\n        't5_summary': t5_summary,\n        't5_rouge1': t5_rouge['ROUGE-1'],\n        't5_rouge2': t5_rouge['ROUGE-2'],\n        't5_rougeL': t5_rouge['ROUGE-L'],\n        't5_avg_rouge': sum(t5_rouge.values()) / 3,\n        'best_model': best['model_name'],\n        'raw_best_summary': best['summary'],\n        'best_avg_score': best['avg_score'],\n        'cleaned_narrative': narrative,\n        'qwen_used': narrative_model is not None,\n        'processing_time_seconds': time.time() - product_start\n    }\n    \n    current_batch.append(result)\n    total_processed += 1\n    \n    # Append to files when batch is full\n    if len(current_batch) >= BATCH_SIZE:\n        print(f\"\\n{'='*80}\")\n        print(f\"SAVING BATCH {batch_count + 1} ({total_processed - len(current_batch) + 1}-{total_processed})\")\n        print(f\"{'='*80}\")\n        is_first = (batch_count == 0)\n        append_to_files(current_batch, is_first_batch=is_first)\n        current_batch = []\n        batch_count += 1\n    \n    # Progress update\n    if total_processed % 10 == 0:\n        elapsed = time.time() - start_total\n        avg_time = elapsed / total_processed\n        remaining = (len(df_with_reviews) - total_processed) * avg_time\n        print(f\"\\n  Progress: {total_processed}/{len(df_with_reviews)} | Avg: {avg_time:.1f}s/product | ETA: {remaining/60:.1f} min\")\n\n# Save final incomplete batch if any\nif len(current_batch) > 0:\n    print(f\"\\n{'='*80}\")\n    print(f\"SAVING FINAL BATCH ({total_processed - len(current_batch) + 1}-{total_processed})\")\n    print(f\"{'='*80}\")\n    is_first = (batch_count == 0)\n    append_to_files(current_batch, is_first_batch=is_first)\n    batch_count += 1\n\n# ============================================================================\n# HANDLE PRODUCTS WITHOUT REVIEWS\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"HANDLING PRODUCTS WITHOUT REVIEWS\")\nprint(\"=\"*80)\n\nif len(df_without_reviews) > 0:\n    no_review_results = []\n    for idx, row in df_without_reviews.iterrows():\n        result = {\n            'name': row['name'],\n            'main_category': row['main_category'],\n            'sub_category': row['sub_category'],\n            'image': row['image'],\n            'link': row['link'],\n            'no_of_ratings': row['no_of_ratings'],\n            'discount_price': row['discount_price'],\n            'actual_price': row['actual_price'],\n            'review_rating': row['review_rating'],\n            'original_review_text': '',\n            'bart_summary': '',\n            'bart_rouge1': 0.0,\n            'bart_rouge2': 0.0,\n            'bart_rougeL': 0.0,\n            'bart_avg_rouge': 0.0,\n            't5_summary': '',\n            't5_rouge1': 0.0,\n            't5_rouge2': 0.0,\n            't5_rougeL': 0.0,\n            't5_avg_rouge': 0.0,\n            'best_model': 'N/A',\n            'raw_best_summary': '',\n            'best_avg_score': 0.0,\n            'cleaned_narrative': 'No customer review available for this product.',\n            'qwen_used': False,\n            'processing_time_seconds': 0.0\n        }\n        no_review_results.append(result)\n    \n    print(f\"Appending {len(no_review_results)} products without reviews...\")\n    is_first = (batch_count == 0)\n    append_to_files(no_review_results, is_first_batch=is_first)\n    total_processed += len(no_review_results)\n\n# ============================================================================\n# DISPLAY FINAL METRICS AND SAMPLES\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"OVERALL METRICS\")\nprint(\"=\"*80)\n\n# Calculate average ROUGE scores\nif len(all_rouge_scores['bart']) > 0:\n    bart_avg_rouge1 = sum(score['ROUGE-1'] for score in all_rouge_scores['bart']) / len(all_rouge_scores['bart'])\n    bart_avg_rouge2 = sum(score['ROUGE-2'] for score in all_rouge_scores['bart']) / len(all_rouge_scores['bart'])\n    bart_avg_rougeL = sum(score['ROUGE-L'] for score in all_rouge_scores['bart']) / len(all_rouge_scores['bart'])\n    \n    t5_avg_rouge1 = sum(score['ROUGE-1'] for score in all_rouge_scores['t5']) / len(all_rouge_scores['t5'])\n    t5_avg_rouge2 = sum(score['ROUGE-2'] for score in all_rouge_scores['t5']) / len(all_rouge_scores['t5'])\n    t5_avg_rougeL = sum(score['ROUGE-L'] for score in all_rouge_scores['t5']) / len(all_rouge_scores['t5'])\n    \n    print(f\"\\nBART: ROUGE-1={bart_avg_rouge1:.4f}, \"\n          f\"ROUGE-2={bart_avg_rouge2:.4f}, \"\n          f\"ROUGE-L={bart_avg_rougeL:.4f}\")\n    print(f\"T5:   ROUGE-1={t5_avg_rouge1:.4f}, \"\n          f\"ROUGE-2={t5_avg_rouge2:.4f}, \"\n          f\"ROUGE-L={t5_avg_rougeL:.4f}\")\n    \n    total_with_reviews = len(all_rouge_scores['bart'])\n    print(f\"\\nBest Model: BART={model_selection_count['BART']} ({model_selection_count['BART']/total_with_reviews*100:.1f}%), \"\n          f\"T5={model_selection_count['T5']} ({model_selection_count['T5']/total_with_reviews*100:.1f}%)\")\nelse:\n    print(\"No products with reviews to calculate metrics\")\n\nprint(f\"\\nQwen Enhancement: {'ENABLED' if narrative_model is not None else 'DISABLED (using simple formatting)'}\")\nprint(f\"Total products processed: {total_processed}\")\nprint(f\"Products with reviews: {len(all_rouge_scores['bart'])}\")\nprint(f\"Products without reviews: {len(df_without_reviews)}\")\nprint(f\"Total batches saved: {batch_count}\")\nprint(f\"Total processing time: {(time.time()-start_total)/60:.1f} min\")\n\n# Get final file sizes\ncomplete_size = os.path.getsize(COMPLETE_RESULTS_FILE) / (1024 * 1024)  # MB\nnarrative_size = os.path.getsize(FINAL_NARRATIVES_FILE) / (1024 * 1024)  # MB\n\nprint(f\"\\n✓ Final files:\")\nprint(f\"  {COMPLETE_RESULTS_FILE}: {complete_size:.2f} MB ({total_processed} rows)\")\nprint(f\"  {FINAL_NARRATIVES_FILE}: {narrative_size:.2f} MB ({total_processed} rows)\")\n\n# Display samples from final file\nprint(\"\\n\" + \"=\"*80)\nprint(\"SAMPLE RESULTS FROM FINAL FILE (First 3 with reviews)\")\nprint(\"=\"*80)\n\ntry:\n    # Read first few rows to display\n    sample_df = pd.read_csv(COMPLETE_RESULTS_FILE, nrows=min(100, total_processed))\n    sample_with_reviews = sample_df[sample_df['bart_summary'] != '']\n    \n    for idx in range(min(3, len(sample_with_reviews))):\n        row = sample_with_reviews.iloc[idx]\n        print(f\"\\n[{idx+1}] {row['name'][:60]}\")\n        print(f\"  Rating: {row['review_rating']} | Price: {row['discount_price']}\")\n        print(f\"  BEST ({row['best_model']}): {row['raw_best_summary'][:100]}...\")\n        print(f\"  NARRATIVE: {row['cleaned_narrative'][:150]}...\")\nexcept Exception as e:\n    print(f\"Could not read sample: {e}\")\n\nprint(f\"\\n{'='*80}\")\nprint(f\"✓ COMPLETE! {total_processed} products processed in {(time.time()-start_total)/60:.1f} min\")\nprint(f\"✓ Files built incrementally and ready to use\")\nif SAMPLE_SIZE: \n    print(f\"⚠️ Set SAMPLE_SIZE=None for full dataset\")\nprint(f\"{'='*80}\")","metadata":{"execution":{"iopub.execute_input":"2025-11-25T17:54:37.43531Z","iopub.status.busy":"2025-11-25T17:54:37.434686Z","iopub.status.idle":"2025-11-26T00:00:51.064467Z","shell.execute_reply":"2025-11-26T00:00:51.063477Z","shell.execute_reply.started":"2025-11-25T17:54:37.435277Z"}},"outputs":[],"execution_count":null}]}